{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPicJjApGWe8+URp/MlcraA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peterliujpdev/MNIST-Digit-Recognizer/blob/main/MNIST_Digit_Recognizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Math Intuition\n",
        "\n"
      ],
      "metadata": {
        "id": "j83-175Sty1X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Pytorch structure"
      ],
      "metadata": {
        "id": "WfTtXh4lxB5Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSAEmXEotYOK",
        "outputId": "8500bf56-5bc3-41d5-f71a-f6d62cdc4d47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimpleCNN(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=1600, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        # 1. Âç∑ÁßØÂ±Ç Convolutional Layers\n",
        "        # ËæìÂÖ•ÈÄöÈÅì 1 (ÈªëÁôΩÂõæ), ËæìÂá∫ÈÄöÈÅì 32 (ÊèêÂèñ32ÁßçÁâπÂæÅ), Âç∑ÁßØÊ†∏ 3x3\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)\n",
        "        # ËæìÂÖ•ÈÄöÈÅì 32, ËæìÂá∫ÈÄöÈÅì 64\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
        "\n",
        "        # 2. ÂÖ®ËøûÊé•Â±Ç Fully Connected Layers\n",
        "        # ËÆ°ÁÆóÁª¥Â∫¶\n",
        "        self.fc1 = nn.Linear(64 * 5 * 5, 128)\n",
        "        self.fc2 = nn.Linear(128, 10) # ËæìÂá∫Êï∞Â≠ó\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Á¨¨‰∏ÄÂ±ÇÂç∑ÁßØ + ÊøÄÊ¥ªÂáΩÊï∞ ReLU + Ê±†Âåñ Pooling\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2) # ÂõæÁâáÂèòÂ∞è‰∏ÄÂçä\n",
        "\n",
        "        # Á¨¨‰∫åÂ±ÇÂç∑ÁßØ\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        # Â±ïÂπ≥ Flatten: ÊääÁ´ã‰ΩìÁöÑÊï∞ÊçÆÊãçÊâÅÊàê‰∏ÄÁª¥ÂêëÈáèÔºåÂáÜÂ§áÂñÇÁªôÂÖ®ËøûÊé•Â±Ç\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        # ÂÖ®ËøûÊé•ÂàÜÁ±ª\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        # ËæìÂá∫ Logits (Êú™ÂΩí‰∏ÄÂåñÁöÑÊ¶ÇÁéá)\n",
        "        return x\n",
        "\n",
        "# ÂÆû‰æãÂåñÊ®°Âûã\n",
        "model = SimpleCNN()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JbnEzufzxPvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# ==========================================\n",
        "# Data Preparation\n",
        "# ==========================================\n",
        "# ÊääÂõæÁâáÂèòÊàê Tensor Áü©ÈòµÔºåÂπ∂ËÆ©Êï∞ÂÄºÂú® 0-1 ‰πãÈó¥\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# ‰∏ãËΩΩËÆ≠ÁªÉÈõÜ (60,000)\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "# ‰∏ãËΩΩÊµãËØïÈõÜ (10,000)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Âª∫Á´ãÂä†ËΩΩÂô® (Batch Size = 64 ÊØèÊ¨°Âπ∂Ë°åËÆ°ÁÆó 64 Âº†ÂõæÔºåÂà©Áî®Áü©ÈòµÂä†ÈÄü)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# ==========================================\n",
        "# Model Architecture\n",
        "# ==========================================\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        # Âç∑ÁßØÂ±ÇÊèêÂèñÁâπÂæÅ\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        # Ê±†ÂåñÂ±ÇÈôçÁª¥ ÈÄâÊúÄÂ§ßÂÄº\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        # ÂÖ®ËøûÊé•Â±ÇÂàÜÁ±ª\n",
        "        # 28x28 ÁöÑÂõæÁªèËøá‰∏§Ê¨°Ê±†ÂåñÂèòÊàê 7x7„ÄÇ\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10) # ËæìÂá∫ 10 ‰∏™Êï∞Â≠óÁöÑÊ¶ÇÁéá\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Layer 1\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        # Layer 2\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        # Flatten\n",
        "        x = x.view(-1, 64 * 7 * 7)\n",
        "        # Fully Connect\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SimpleCNN().to(device)\n",
        "\n",
        "# ==========================================\n",
        "# Math Setup\n",
        "# ==========================================\n",
        "# Loss Function: CrossEntropy\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Optimizer: SGD ÁöÑËøõÂåñÁâà Adam\n",
        "# 0.001 Learning Rate\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# ==========================================\n",
        "# The Training Loop\n",
        "# ==========================================\n",
        "print(\"üöÄ Training Starts... \")\n",
        "\n",
        "num_epochs = 3 # ÊääÊâÄÊúâÊï∞ÊçÆÁúã 3 ÈÅç\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # 1. Reset Gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 2. Forward Pass\n",
        "        outputs = model(images)\n",
        "\n",
        "        # 3. Calculate Loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # 4. Backward Pass - Chain Rule\n",
        "        # ÊØè‰∏™ÂèÇÊï∞ÂØπËØØÂ∑ÆÁöÑË¥°ÁåÆ\n",
        "        loss.backward()\n",
        "\n",
        "        # 5.Update Weights\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "print(\"‚úÖ Training CompleteÔºÅ\")\n",
        "\n",
        "# ==========================================\n",
        "# Evaluation\n",
        "# ==========================================\n",
        "model.eval() # ÊµãËØïÊ®°Âºè\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad(): # ÊµãËØï‰∏çÈúÄË¶ÅÁÆóÊ¢ØÂ∫¶\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1) # ÈÄâÊ¶ÇÁéáÊúÄÂ§ßÁöÑÈÇ£‰∏™Êï∞Â≠ó\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'üéâ Âú® 10,000 Âº†ÊµãËØïÂõæÁâá‰∏äÁöÑÂáÜÁ°ÆÁéá: {100 * correct / total:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGlqmTXYxXH0",
        "outputId": "0d2d8880-368e-4c38-8117-3e04f06d8d92"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Training Starts... \n",
            "Epoch [1/3], Step [100/938], Loss: 0.2996\n",
            "Epoch [1/3], Step [200/938], Loss: 0.1193\n",
            "Epoch [1/3], Step [300/938], Loss: 0.0573\n",
            "Epoch [1/3], Step [400/938], Loss: 0.0758\n",
            "Epoch [1/3], Step [500/938], Loss: 0.0304\n",
            "Epoch [1/3], Step [600/938], Loss: 0.0617\n",
            "Epoch [1/3], Step [700/938], Loss: 0.1924\n",
            "Epoch [1/3], Step [800/938], Loss: 0.0972\n",
            "Epoch [1/3], Step [900/938], Loss: 0.0300\n",
            "Epoch [2/3], Step [100/938], Loss: 0.0419\n",
            "Epoch [2/3], Step [200/938], Loss: 0.0652\n",
            "Epoch [2/3], Step [300/938], Loss: 0.1928\n",
            "Epoch [2/3], Step [400/938], Loss: 0.0124\n",
            "Epoch [2/3], Step [500/938], Loss: 0.1445\n",
            "Epoch [2/3], Step [600/938], Loss: 0.1554\n",
            "Epoch [2/3], Step [700/938], Loss: 0.0083\n",
            "Epoch [2/3], Step [800/938], Loss: 0.0067\n",
            "Epoch [2/3], Step [900/938], Loss: 0.0350\n",
            "Epoch [3/3], Step [100/938], Loss: 0.0385\n",
            "Epoch [3/3], Step [200/938], Loss: 0.0061\n",
            "Epoch [3/3], Step [300/938], Loss: 0.0345\n",
            "Epoch [3/3], Step [400/938], Loss: 0.0086\n",
            "Epoch [3/3], Step [500/938], Loss: 0.0023\n",
            "Epoch [3/3], Step [600/938], Loss: 0.0393\n",
            "Epoch [3/3], Step [700/938], Loss: 0.0404\n",
            "Epoch [3/3], Step [800/938], Loss: 0.0558\n",
            "Epoch [3/3], Step [900/938], Loss: 0.0053\n",
            "‚úÖ Training CompleteÔºÅ\n",
            "üéâ Âú® 10,000 Âº†ÊµãËØïÂõæÁâá‰∏äÁöÑÂáÜÁ°ÆÁéá: 98.96%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hfX_SAIjzFP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageOps\n",
        "from google.colab import files\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def predict_my_image():\n",
        "    # 1. ÂºπÂá∫‰∏ä‰º†ÊåâÈíÆ\n",
        "    print(\"Press the button to upload any number pictures (black number with white background)...\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    for filename in uploaded.keys():\n",
        "        # 2. ÊâìÂºÄÂõæÁâá\n",
        "        image = Image.open(filename)\n",
        "\n",
        "        # 3. ÂõæÁâáÈ¢ÑÂ§ÑÁêÜ\n",
        "        # 3.1 ËΩ¨‰∏∫ÁÅ∞Â∫¶Âõæ (RGB -> Gray)\n",
        "        img_gray = image.convert('L')\n",
        "\n",
        "        # 3.2 È¢úËâ≤ÂèçËΩ¨ (MNIST ‰∏∫ÈªëÂ∫ïÁôΩÂ≠ó)\n",
        "        img_inverted = ImageOps.invert(img_gray)\n",
        "\n",
        "        # 3.3 Ë∞ÉÊï¥Â§ßÂ∞è‰∏∫ 28x28\n",
        "        img_resized = img_inverted.resize((28, 28))\n",
        "\n",
        "        # 3.4 ËΩ¨‰∏∫ Tensor Âπ∂Â¢ûÂä†Áª¥Â∫¶ ([1, 1, 28, 28])\n",
        "        img_tensor = transform(img_resized).unsqueeze(0).to(device)\n",
        "\n",
        "        # 4. ËÆ©Ê®°ÂûãÈ¢ÑÊµã\n",
        "        model.eval() # ÂàáÊç¢Âà∞ËØÑ‰º∞Ê®°Âºè\n",
        "        with torch.no_grad():\n",
        "            output = model(img_tensor)\n",
        "            # Softmax ËÆ°ÁÆóÊ¶ÇÁéá\n",
        "            probabilities = F.softmax(output, dim=1)\n",
        "            prediction = torch.argmax(probabilities, dim=1).item()\n",
        "            confidence = probabilities[0][prediction].item() * 100\n",
        "\n",
        "        # 5. print\n",
        "        plt.figure(figsize=(4, 4))\n",
        "        plt.imshow(img_resized, cmap='gray')\n",
        "        plt.title(f\"AI Guess: {prediction} ({confidence:.2f}%)\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "# Finally predict\n",
        "predict_my_image()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "id": "TJvxHgBezFaj",
        "outputId": "6c6b9e07-5056-4c7e-8803-8d5a8e16aeb6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Press the button to upload any number pictures (black number with white background)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bd5f93de-55fd-4fd7-a644-3319c36b6088\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bd5f93de-55fd-4fd7-a644-3319c36b6088\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving image6.jpg to image6 (2).jpg\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAAFeCAYAAADnm4a1AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAG69JREFUeJzt3XlwleX5xvH7ZF9YEhIIEBXBpdjRIlACBVEEgsWtYhTZBIShLCoqMiMtpYDQWivgVnBhBKNlSq2auhQqTBAXhmERxJGGGpWCC0uCwUCAhCT37w8nZ3pMbnge9DUn/X0/M5mR91zn5Tkn51y85nDzhFRVBQBQT0xjLwAAohUFCQAGChIADBQkABgoSAAwUJAAYKAgAcBAQQKAgYIEAAMFif8Jn332mSQlJcmGDRsaeylRYdiwYTJ06NDGXkaTR0F+R0uWLJFQKCQ9e/Y0M6FQSO644w6n89XW1spzzz0nubm5kpmZKfHx8dKmTRsZNGiQPP3001JZWfl9LT2q1NbWyhNPPCGXXnqpJCcnS0ZGhvTv31927NjhdP/7779fevbsKX369Ik4vnLlSunWrZskJSVJ69atZfz48VJaWnrKc7377rsSCoUkFAqdNisisnPnTrn55pulU6dOkpKSIpmZmXL55ZfLa6+9Vi9bd96GvnJzc8O5w4cPy8iRIyU9PV06deokzzzzTL1zbd26VVJSUmT37t31brvvvvvkpZdecn7+0LC4xl5AU7dixQo599xzZfPmzfLxxx/L+eeff8bnOn78uAwZMkTeeOMN6d27t0yfPl2ysrLkq6++krfeekumTJkimzZtavDN0tSNGzdOVqxYIaNHj5Y77rhDKioqZPv27XLw4MHT3rekpETy8/MlPz8/4vgTTzwhU6ZMkQEDBsiiRYvk888/l0cffVS2bt0qmzZtkqSkpHrnqq2tlTvvvFNSU1OloqLCae179uyRI0eOyJgxY6R9+/Zy7Ngxeemll+T666+Xp556Sn75y1+Gs88//3y9+2/dulUeffRRGTRoUPjY9OnTZf369TJ37lz5+OOPZcKECXLRRRdJ7969RUREVWXq1Kly9913S8eOHeuds2vXrvLTn/5UFi5cKM8995zT40ADFGfs008/VRHRl19+WVu3bq1z5sxpMCcievvtt5/2fBMnTlQR0UceeaTB2z/66CNdvHjxd1pzNPrrX/8afh7PxKJFizQ5OVmPHDkSPlZZWalpaWl6+eWXa21tbfj4a6+9piKijz32WIPneuKJJzQjI0PvuusuFREtKSk5ozVVV1drly5d9Ec/+tFps+PHj9dQKKSfffZZ+FhWVpbm5+eHf33FFVfojBkzwr9+/vnntX379hGP+dsWLFigqampp8zg1CjI72DevHmanp6ulZWVOnnyZL3gggsazLkU5N69ezU2NlZ//vOfO//+b775poqIvvnmmxHHd+/erSKiy5cvjzheVFSkeXl5mp6eromJidq9e3d95ZVXIjJVVVU6Z84cPf/88zUxMVFbtWqlffr00TVr1oQz+/bt07Fjx2p2drYmJCRo27Zt9frrr9fdu3eHM4cPH9aioiI9fPjwaR9Hz549NScnR1VVa2pq9OjRo87Pgarq5Zdfrv369Ys49t5776mINPgHSrNmzbR37971jh86dEgzMjJ08eLFOnv27O9UkKqq1157rWZlZZ0yc+LECU1LS6u3/hYtWuirr74a/vWQIUN06tSpqqp69OhRzc7OjijQhuzYseM7/cEDVX4G+R2sWLFCbrzxRklISJDhw4dLcXGxbNmy5YzOtXr1aqmpqZFRo0Z9z6v8xs6dO6VXr15SVFQkM2bMkIULF0pqaqrccMMNUlBQEM7NmTNH5s6dK1deeaX86U9/kpkzZ8o555wj27ZtC2fy8vKkoKBAbrvtNlmyZIlMnTpVjhw5Inv37g1nCgoK5KKLLoo4d0PKy8tl8+bN0qNHD/n1r38tLVu2lGbNmkmnTp3khRdeOO3jOnnypGzZskW6desWcbzuZ7XJycn17pOcnCzbt2+X2traiOOzZs2Stm3bysSJE0/7+zakoqJCSktL5ZNPPpGHH35YVq9eLQMGDDjlfVatWhX+eeN/69GjhyxatEiKi4vljTfekH/+85+Sk5MjIiK///3vJTs7W2699dZTnvvHP/6xJCcn88HVd9HYDd1Ubd26VUVE165dq6qqtbW1etZZZ+ldd91VLysOV5D33HOPioi+//77EccrKyu1pKQk/FVaWhq+zecKcsCAAXrJJZfoiRMnwsdqa2u1d+/eEVe+Xbp00WuuucZcZ1lZmYqIPvTQQ6d8PMuXL2/wKvbbtm3bpiKiGRkZmpWVpUuWLNEVK1ZoTk6OhkIhXb169Snv//HHH6uI6OOPPx5xvKSkREOhkI4fPz7i+K5du1REVEQinssdO3ZobGysvvHGG6qqZ3QFWfcjEhHRmJgYvemmm/Srr7465X3y8vI0MTFRy8rKIo5/8MEHetZZZ4XPl5eXpzU1Nfrpp59qcnKybty40WlNF154oQ4ePNj5MSASV5BnaMWKFZKVlSVXXnmliHzz6eQtt9wiK1eulJqaGu/zlZeXi4hIs2bNIo6vWrVKWrduHf7q0KGD97m/+uorWbdunQwdOlSOHDkipaWlUlpaKocOHZKrrrpKiouL5YsvvhARkbS0NNm5c6cUFxc3eK7k5GRJSEiQ9evXS1lZmfl7jh07VlRVxo4de8q1HT16VEREDh06JK+88opMnjxZRowYIYWFhZKRkSHz588/5f0PHTokIiLp6ekRxzMzM2Xo0KGSn58vCxculE8//VTeeecdueWWWyQ+Pl5EvvlQrM7UqVNl8ODBER+U+Lr77rtl7dq1kp+fL4MHD5aamhqpqqoy8+Xl5fKPf/xDrr76aklLS4u47ZJLLgn/H0lxcbG8+OKLEhMTI/fee6/k5eVJr1695OWXX5YuXbpIx44d5f777xdt4N++Tk9Pd/okHobGbuimqLq6Wtu1a6fDhg3T4uLi8NcLL7ygIhK+CqkjDleQd999d4NXkAcPHtS1a9fq2rVrddCgQZqamhq+zfUKctOmTeErEetr27Ztqqr61ltvaVpamoqIXnzxxTp9+nTdsWNHxPkffvhhjYmJ0fj4eO3bt68++OCDum/fPp+nMGzLli0qItqxY8d6t912220aHx+vJ0+eNO9f99ief/75ercdPnxYr7/++ojHOWrUKL3xxhtVRMJXbStXrtT4+Hj997//Hb7v9/EzyNzcXO3Ro0fEh0T/bdmyZSoi+uKLLzqdr7CwUFNTU/Xzzz/XXbt2aXx8vC5btkzXrVunWVlZumzZsnr3ycnJCf98F/4oyDOwZs2aU5bN6NGjI/IuBfnkk0+qiOif//xnMzNmzJiIgly/fn2DBVn3v511Bblx40YVEZ0+fXq4bL/9VV5eHr7/oUOHdNmyZTps2DBNS0vT2NhYXbp0ab3fY8GCBZqbm6sJCQmalpYWLlkfX3zxhYqI9urVq95t9913n4rIKT/oqXus1qfSqqp79uzRt956S//zn/+oqurPfvYzbd26dfj2s88+W0eMGKG7d+8Of9V9ir1t2zb94osvvB+XqupTTz2lIqK7du1q8PYBAwZoy5YtI37sYamurtaLL75Y582bp6qq999/v15xxRXh23/729/qgAED6t3vggsu0KuvvvqM1g8K8oyMGTNG27Rpo3/729/qfQ0fPlybN2+ux44dC+ddCnLPnj0aGxt7yp8Xfbsg6z6lLCgoiMgVFhZGFOSBAwdURPRXv/qV92M9cuSIdu3aVbOzs83MRx99pCkpKTpy5Ejv86uqtm3bVs8+++x6x2+99VZNSkrSmpoa875VVVWanJys99xzj9PvVVZWpgkJCTp8+PDwsdNdXXfp0sX7MamqPvLIIyoiumnTpnq3ffnllxoTE6Pjxo1zOtfjjz+uHTp00OPHj6uq6qRJkyIew5NPPqmdO3eOuM/Jkyc1KSlJ77333jNaPyhIb8eOHdPmzZubL+wNGzaoiOjKlSvDx1wKUlV1woQJDX7gUGf06NERBXn48GGNjY2tVw55eXn1PiDp16+ftmrVSr/88st65z148GD4v//7g4s6N998s2ZmZqqqakVFRfhNWqempkazsrL0pptuilib61/zqbta+++/SlRSUqItWrRwuvrp27ev9u3b97Q51W+KJSYmRjdv3hw+VlBQUO/rlltuURHR5557TtetWxexrqKiIq2oqAgfO3DgQL3fp6qqSrt161bv72fWWbRokYqIFhYWnnbNhw4d0latWukLL7wQPjZ79mzt2rVr+Nd33XVXvb8qVPcH6EsvvXTa3wMNoyA9rVy5UkVE//73vzd4e01NjbZu3Vqvu+668DHXgqyoqNCBAweqiGifPn30D3/4gy5btkz/+Mc/6i9+8QuNiYnRiy66KOI+w4YN07i4OJ02bZouXrxYBw8erN27d69XkDt37tT09HTNyMjQGTNm6NNPP63z5s3Tq6++Wn/yk5+Ec23atNGhQ4fqgw8+qEuXLtWJEydqKBTSO++8U1VVt2/frq1atdJJkybpY489pkuWLNHc3Nx6P0tz/RRbVXX//v3arl07bd68uc6ePVsXLVqkF154oSYnJ9f7mWxDFixYoImJifr1119HHH/ggQd05MiR4XUOGjRIRUTnz59/2nNaP4OsO/7fP9a44YYbtH///jpnzhxdunSpzps3Tzt37qwiogsXLmzw/N27d9f27duf8uq4zpQpUyL+d1r1m0+5Q6GQTpo0SR944AFNSkrSJUuWRGQWLFigKSkpET8+gR8K0tN1112nSUlJEVcQ3zZ27FiNj48PX425FqTqNz9rWr58ufbv319btWqlcXFxmpmZqQMGDNAnn3yy3tVbSUmJ5uXlaUpKiqanp+vEiRP1ww8/bLCcPvnkEx09erS2bdtW4+PjNTs7W6+99tqIYps/f77m5ORoWlqaJicna+fOnfV3v/udVlVVqeo3V5i33367du7cWVNTU7Vly5bas2fPiKsbVb+CrFvbkCFDtEWLFpqcnKz9+/ePuMo7lQMHDmhcXFy9D2pef/11zcnJ0ebNm2tKSor26tWr3jotPgX5l7/8RQcOHKhZWVkaFxen6enpOnDgwHp/Cb9O3V81mjZt2mnX8cEHH2hCQoJu37693m3PPvusnnvuuZqRkaHTpk3T6urqiNt79uypo0aNOv2DhSmkyr7YaPrGjx8vH330kbzzzjuNvZSo8P7770u3bt1k27Ztcumllzb2cposChL/E/bu3SsXXnihFBYW1vsXff4/GjZsmNTW1jpNI8FGQQKAgUkaADBQkABgoCABwEBBAoCBggQAg/OeNC1btnQ+aTR8MB7UGr79j6x+X2JigvmzKhQKBXLeoNYQ1PMbFJ/1BvWa9Dmvzz/F55Ntat831+eMK0gAMFCQAGCgIAHAQEECgIGCBAADBQkABgoSAAwUJAAYKEgAMFCQAGBwHjX0GWfyGS0LavwqqBG7oEYCffg8tmgYNYyG0dNo4PM8+IzuVVdXB7KGoETDa9JV47/bASBKUZAAYKAgAcBAQQKAgYIEAAMFCQAGChIADBQkABgoSAAwUJAAYHAeNQxKUxsJDGr3tmgY5fQ5r8/z67M7ng+f9Qa1hqDGB6Nhl0Cf5zc2NjbAlbgJoku4ggQAAwUJAAYKEgAMFCQAGChIADBQkABgoCABwEBBAoCBggQAAwUJAAbnUcNoGCWKhvGroHaFa2rPb2VlZSBriItzn3712c3PZ70+Y5Q+6w1q/LUp7RLoq7F3SOUKEgAMFCQAGChIADBQkABgoCABwEBBAoCBggQAAwUJAAYKEgAMFCQAGBp9V0MfQY3j+ex4F9QaghoXC+qxXXPNNc7Z3Nxc56yP0tJS52xJSYlzduPGjc7Z9957zznrI6iRy6AENYLrg10NAeAHREECgIGCBAADBQkABgoSAAwUJAAYKEgAMFCQAGCgIAHAQEECgMF5nul/eee0aNhR0IfP+GBSUpJzdtasWc7Zyy67zDm7fPly5+zbb7/tnD158qRz9vzzz3fO9u3b1zn7ySefOGfLysqcsz7vt6B2S/TZ5TIa1huEprNSAPiBUZAAYKAgAcBAQQKAgYIEAAMFCQAGChIADBQkABgoSAAwUJAAYAip43ZkGRkZzif12eEsqBHGoMaZfMb8osHcuXOds127dnXO+owlbtiwwTnr83rwGRH1GUv0GbFLSEhwzsbHxztnfV5nQe0o6LOGaBgf9Pm+VVZWOuUa/1EBQJSiIAHAQEECgIGCBAADBQkABgoSAAwUJAAYKEgAMFCQAGCgIAHA4Lyroc8oUVDjg0GNVAW1e5uPEydOOGdvvvlm5+yQIUOcs7/5zW+csz67D6ampjpng+IzlugzYhcX5/wWCmyM0ud94fNaj4bxQR9BrLdpPQMA8AOiIAHAQEECgIGCBAADBQkABgoSAAwUJAAYKEgAMFCQAGCgIAHA4LyrYZs2bYJey/cqGsYSfdbQrFkz5+yKFSucs//617+cs9OmTXPO+ozYRYOgxkl9sj5r8OFz3qBGGIPa7TOo9/Hx48edclxBAoCBggQAAwUJAAYKEgAMFCQAGChIADBQkABgoCABwEBBAoCBggQAQyDzYj7jV0GNEvkIagTs5MmTztmBAwc6Z5s3b+6cffbZZ52zTY3P2FxQI3Y+gnqdBbX7YFDrDUoQ6+UKEgAMFCQAGChIADBQkABgoCABwEBBAoCBggQAAwUJAAYKEgAMFCQAGAIZNYyG8UEfPmNoPlJSUpyzPqOGH374oXN2165dzlmf75vPGGV8fLxz1me3xOrqaudsUOv1GfPzGcENSlDvzWh4bEGMXHIFCQAGChIADBQkABgoSAAwUJAAYKAgAcBAQQKAgYIEAAMFCQAGChIADIGMGvoIagfEaNjprUOHDs7ZCy64wDn78MMPO2d9xh2vuuoq52yXLl2cs23atHHOpqWlOWcrKiqcs++++65z9pVXXnHOHjt2zDnr85r0ea03tdHepoQrSAAwUJAAYKAgAcBAQQKAgYIEAAMFCQAGChIADBQkABgoSAAwUJAAYHAeNfTZ+S+o8UGfMb+gRg2rqqqcs127dnXO+uzmd8455zhn8/PznbM+Y3OFhYWBZH3GB31GGPv16+ec7d69u3P2oYcecs7u27fPOevzevB5v/m8h4LSlHZW5AoSAAwUJAAYKEgAMFCQAGCgIAHAQEECgIGCBAADBQkABgoSAAwUJAAYQuo495OdnR3IAoIaOwpqhLG6uto5O3PmTOfskCFDnLNFRUXO2aVLlzpnN2/e7JwtLy93zvrwGRerqalxzvq8HkaNGuWc7d+/v3N21qxZztn9+/c7Z4Ma7fXh8x7yWUMQ44Mi7iOtXEECgIGCBAADBQkABgoSAAwUJAAYKEgAMFCQAGCgIAHAQEECgIGCBACD+9ZpHqJh90GfEaWg1nDxxRc7Zzt27Oic9dlJz2dHQZ+dK+Pj452zQY2L+azB5zW5cuVK52zbtm2ds1OmTHHOzpkzxznrI6gdEH3eQ0GNJQaBK0gAMFCQAGCgIAHAQEECgIGCBAADBQkABgoSAAwUJAAYKEgAMFCQAGAIZNQwqNG9oPiMPiUmJjpnO3To4Jzdu3evc3bNmjXO2bg492+xz6ihD5/nNyhBvSafeeYZ5+yDDz7onG3Xrp1z9ssvv3TO+owaRsNuiY29hqbVZADwA6IgAcBAQQKAgYIEAAMFCQAGChIADBQkABgoSAAwUJAAYKAgAcDgPIfmM4bmM/ITDTunRYN3333XOVtSUuKc9RmNDGpcLBp2rvR5bD7rPXbsmHO2rKzMOeuzy6XPmGo07EYZ1Ps4iPVyBQkABgoSAAwUJAAYKEgAMFCQAGCgIAHAQEECgIGCBAADBQkABgoSAAyB7GrY1PiMKJ04ccI5u2vXLufs/v37nbM+ghoXC2osMRoE9Zz5jA/27dvXOVtYWOic9Rk1bGrY1RAAfkAUJAAYKEgAMFCQAGCgIAHAQEECgIGCBAADBQkABgoSAAwUJAAYnEcNm9qOdz58Rstqamqcs5s2bXLODh482DmblJTknI0GQe0+GNTumT7ZtLQ05+x5553nnF21apVz1mfHUR+NvaOgL3Y1BIAfEAUJAAYKEgAMFCQAGChIADBQkABgoCABwEBBAoCBggQAAwUJAAbnUUOfMZ6gRst8xvx8+Iw7JiQkOGcLCgqcs3l5ec7Z3Nxc5+yrr77qnE1OTnbO+oy3+YysBcXne3zs2DHn7PDhw52zBw8edM6+/vrrztmgdioMagzY5z3f2KPIXEECgIGCBAADBQkABgoSAAwUJAAYKEgAMFCQAGCgIAHAQEECgIGCBACD86ihj6B2QPQZb4uGXRh9RsvmzZvnnJ09e7Zztrq62jm7Zs0a56yPoHa88/kenzx50jnrMz44YsQI5+yECROcs2VlZc7ZoEYNg9o1Mhp2QHTFFSQAGChIADBQkABgoCABwEBBAoCBggQAAwUJAAYKEgAMFCQAGChIADCE1HGeqEOHDs4nDWqnwqB2OAtq1z2f56GystI5e+WVVzpnZ86c6Zzdu3evczY/P985W1RU5Jw9ceKEczYtLc05O27cOOeszw6TkydPds6uXbvWOZuYmOicDUo07Ebp8x7yyR45csQpxxUkABgoSAAwUJAAYKAgAcBAQQKAgYIEAAMFCQAGChIADBQkABgoSAAwOI8aduzY0fmkPiNKQe0+GJSg1uszcukzjpedne2cnTp1qnP2sssuc876jIj67FyZkpLinPUZo5w7d65z9u2333bOJiUlOWd9BPWajIb3ZlDjjl9//bVTjitIADBQkABgoCABwEBBAoCBggQAAwUJAAYKEgAMFCQAGChIADBQkABgaPRRQx/RMFIVCoWcsz7jg0Ht3lZdXR1INjMz0zl73nnnOWd9ds88cOCAc3bTpk3O2aNHjzpnExISnLM+Y5RBvXZ8+Jw3qPFin/ebD0YNAeA7oiABwEBBAoCBggQAAwUJAAYKEgAMFCQAGChIADBQkABgoCABwNDoo4Y+O94FNc4U1KhWU9vd0We8zeex+ZzXJ+sjLi4ukPP6COq1HtRor4+mNl5cXl7ulOMKEgAMFCQAGChIADBQkABgoCABwEBBAoCBggQAAwUJAAYKEgAMFCQAGAKZv/IZqQpKULuh+YzCRcMYpY+gvm8+34v4+HjnbFMbJw1qHC+o17rPcxbUa6ex3xeN32QAEKUoSAAwUJAAYKAgAcBAQQKAgYIEAAMFCQAGChIADBQkABgoSAAwNPpWb9GwI1s07Cjow2e0rKmNJQa1q2FQO20GNU4Kf4waAsAPiIIEAAMFCQAGChIADBQkABgoSAAwUJAAYKAgAcBAQQKAgYIEAEMgo4ZBjVT5jHX5jKz5jO7FxsY6Z31Ew65wQfF5foPaxS6o75uPoMYSg3p+g3rt+HzffB5bELs7cgUJAAYKEgAMFCQAGChIADBQkABgoCABwEBBAoCBggQAAwUJAAYKEgAMIW1qW/oBwA+EK0gAMFCQAGCgIAHAQEECgIGCBAADBQkABgoSAAwUJAAYKEgAMPwfBhB+n/0z1fwAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}